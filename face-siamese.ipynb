{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f11e373-8405-4d41-ac9f-f95f0a389520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7c68ea-8a7d-4bef-a9ff-0852d5b00c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder structure\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c6521-291b-4a8d-affa-a7d834211594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directories\n",
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69f9c5-1297-4472-a243-3807cea775ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset - https://www.kaggle.com/datasets/jessicali9530/lfw-dataset?resource=download\n",
    "#extracting lfw dataset\n",
    "!tar -xf lfw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5c8a7-b8f9-4720-8e09-577c2153dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move lfw images to data/negative\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(EX_PATH, NEW_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84df9897-2584-4fce-969c-9a64ac375e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uuid to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b644c6c0-94d6-4ecd-a254-850a410c0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image capturing for anchors, positives\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    #cut down frame to 250x250\n",
    "    frame = frame = frame[120:120+250, 200:200+250, :]\n",
    "\n",
    "    #collect anchors\n",
    "    if cv2.waitKey(1) & 0xFF == ord('a'):\n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        \n",
    "    #collect positives\n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'):\n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    cv2.imshow('Image Collection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17ef2ab-ca54-4890-917f-29641e63d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data paths\n",
    "anchor = [os.path.join(ANC_PATH, f) for f in os.listdir(ANC_PATH) if f.endswith(\".jpg\")][:300]\n",
    "positive = [os.path.join(POS_PATH, f) for f in os.listdir(POS_PATH) if f.endswith(\".jpg\")][:300]\n",
    "negative = [os.path.join(NEG_PATH, f) for f in os.listdir(NEG_PATH) if f.endswith(\".jpg\")][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfda50e9-ab56-492a-838e-b98f5a2feb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_and_preprocess(file_path):\n",
    "    img = Image.open(file_path).convert(\"RGB\")\n",
    "    img = preprocess(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2936da98-61f4-4616-a674-64aeb151cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = load_and_preprocess('data\\\\anchor\\\\74e85329-d280-11f0-a34d-80a3977a1ccc.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07534852-3d9a-4eaa-942e-aab5c92df47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a726a5-4da5-4600-a0f8-5c4bf6a73789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating labelled dataset\n",
    "positives = [(a, p, 1) for a, p in zip(anchor, positive)]\n",
    "negatives = [(a, n, 0) for a, n in zip(anchor, negative)]\n",
    "\n",
    "data = positives + negatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ad78c9-2218-4504-80d6-3567cba800df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset\n",
    "random.shuffle(data)\n",
    "\n",
    "split = int(0.7 * len(data))\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8bf7b9-4494-48e4-91ca-19879be6b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path, label = self.pairs[idx]\n",
    "        img1 = load_and_preprocess(img1_path)\n",
    "        img2 = load_and_preprocess(img2_path)\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "208860ee-aa45-4add-b36e-5e1c81518701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "train_dataset = SiameseDataset(train_data)\n",
    "test_dataset = SiameseDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "876d3fd6-8c69-4f27-97f8-2f4de5faf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5deaf4a3-c8c9-42c4-90cd-7fa08fcd3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding layer\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #first block\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=10)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #second block\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=7)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #third block\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=4)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #final embedding block\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4)\n",
    "\n",
    "        #fully connected linear layer\n",
    "        self.fc = nn.Linear(256 * 5 * 5, 4096)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1) #flatten\n",
    "        x = torch.sigmoid(self.fc(x)) #dense (4096, sigmoid)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b8df74-e526-4e54-9993-36997209df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = EmbeddingNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72bd29b-4dff-40ca-810c-e5593a5d4c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 91, 91]          19,264\n",
      "         MaxPool2d-2           [-1, 64, 45, 45]               0\n",
      "            Conv2d-3          [-1, 128, 39, 39]         401,536\n",
      "         MaxPool2d-4          [-1, 128, 19, 19]               0\n",
      "            Conv2d-5          [-1, 128, 16, 16]         262,272\n",
      "         MaxPool2d-6            [-1, 128, 8, 8]               0\n",
      "            Conv2d-7            [-1, 256, 5, 5]         524,544\n",
      "            Linear-8                 [-1, 4096]      26,218,496\n",
      "================================================================\n",
      "Total params: 27,426,112\n",
      "Trainable params: 27,426,112\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 7.26\n",
      "Params size (MB): 104.62\n",
      "Estimated Total Size (MB): 112.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(embedding, input_size=(3, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ab432db-b2c5-4e14-b77c-472abb926d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#siamese l1 distance layer\n",
    "class L1Dist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_embedding, validation_embedding):\n",
    "        return torch.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9a3822e-d10e-4e1f-8fad-27b6abb566e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#siamese model\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = embedding_model\n",
    "        self.l1 = L1Dist()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        #embeddings\n",
    "        emb1 = self.embedding(img1)\n",
    "        emb2 = self.embedding(img2)\n",
    "\n",
    "        #L1 distance\n",
    "        distance = self.l1(emb1, emb2)\n",
    "\n",
    "        #classifier\n",
    "        output = self.classifier(distance)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7f97d10-7646-4646-b1fa-471110c5aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final siamese model\n",
    "siamese_model = SiameseNetwork(embedding_model=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdf190cd-6eb6-418c-b987-c0135c3b96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "criterion = nn.BCELoss()\n",
    "#optimizer\n",
    "optimizer = optim.Adam(siamese_model.parameters(), lr=0.0001)\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11e64d59-d0cf-4d9d-983c-ad4da92f24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create checkpoint directory\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5a9edb0-4551-4ee6-9ee0-320aa72f6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving checkpoint\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': siamese_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, \"training_checkpoints/ckpt\")\n",
    "\n",
    "checkpoint = torch.load(\"training_checkpoints/ckpt\")\n",
    "siamese_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f7296ff-befa-4c94-b5f9-511ff65a3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training \n",
    "def train_step(batch, model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    img1, img2, labels = batch      #unpacking\n",
    "    img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()         \n",
    "\n",
    "    outputs = model(img1, img2)     #forward pass \n",
    "\n",
    "    loss = criterion(outputs, labels.unsqueeze(1))   #BCE loss requires shape (B,1)\n",
    "    print(loss)\n",
    "    \n",
    "    loss.backward()               \n",
    "    optimizer.step()                \n",
    "\n",
    "    return loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
